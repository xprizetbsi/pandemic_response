{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 (c) Cognizant Digital Business, Evolutionary AI. All rights reserved. Issued under the Apache 2.0 License.\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-daeba1b997f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mROOT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mMODEL_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDATA_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OxCGRT_latest.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ID_COLS = ['CountryName',\n\u001b[1;32m      5\u001b[0m            \u001b[0;34m'RegionName'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "MODEL_FILE = os.path.join(ROOT_DIR, \"models\", \"model.pkl\")\n",
    "DATA_FILE = os.path.join(ROOT_DIR, 'data', \"OxCGRT_latest.csv\")\n",
    "ID_COLS = ['CountryName',\n",
    "           'RegionName',\n",
    "           'GeoID',\n",
    "           'Date']\n",
    "CASES_COL = ['NewCases']\n",
    "NPI_COLS = ['C1_School closing',\n",
    "            'C2_Workplace closing',\n",
    "            'C3_Cancel public events',\n",
    "            'C4_Restrictions on gatherings',\n",
    "            'C5_Close public transport',\n",
    "            'C6_Stay at home requirements',\n",
    "            'C7_Restrictions on internal movement',\n",
    "            'C8_International travel controls',\n",
    "            'H1_Public information campaigns',\n",
    "            'H2_Testing policy',\n",
    "            'H3_Contact tracing',\n",
    "            'H6_Facial Coverings']\n",
    "NB_LOOKBACK_DAYS = 30\n",
    "# For testing, restrict training data to that before a hypothetical predictor submission date\n",
    "HYPOTHETICAL_SUBMISSION_DATE = np.datetime64(\"2020-07-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(start_date: str,\n",
    "            end_date: str,\n",
    "            path_to_ips_file: str,\n",
    "            output_file_path) -> None:\n",
    "    \"\"\"\n",
    "    Generates and saves a file with daily new cases predictions for the given countries, regions and intervention\n",
    "    plans, between start_date and end_date, included.\n",
    "    :param start_date: day from which to start making predictions, as a string, format YYYY-MM-DDD\n",
    "    :param end_date: day on which to stop making predictions, as a string, format YYYY-MM-DDD\n",
    "    :param path_to_ips_file: path to a csv file containing the intervention plans between inception date (Jan 1 2020)\n",
    "     and end_date, for the countries and regions for which a prediction is needed\n",
    "    :param output_file_path: path to file to save the predictions to\n",
    "    :return: Nothing. Saves the generated predictions to an output_file_path CSV file\n",
    "    with columns \"CountryName,RegionName,Date,PredictedDailyNewCases\"\n",
    "    \"\"\"\n",
    "    # !!! YOUR CODE HERE !!!\n",
    "    preds_df = predict_df(start_date, end_date, path_to_ips_file, verbose=False)\n",
    "    # Create the output path\n",
    "    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "    # Save to a csv file\n",
    "    preds_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved predictions to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_df(start_date_str: str, end_date_str: str, path_to_ips_file: str, verbose=False):\n",
    "    \"\"\"\n",
    "    Generates a file with daily new cases predictions for the given countries, regions and npis, between\n",
    "    start_date and end_date, included.\n",
    "    :param start_date_str: day from which to start making predictions, as a string, format YYYY-MM-DDD\n",
    "    :param end_date_str: day on which to stop making predictions, as a string, format YYYY-MM-DDD\n",
    "    :param path_to_ips_file: path to a csv file containing the intervention plans between inception_date and end_date\n",
    "    :param verbose: True to print debug logs\n",
    "    :return: a Pandas DataFrame containing the predictions\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(start_date_str, format='%Y-%m-%d')\n",
    "    end_date = pd.to_datetime(end_date_str, format='%Y-%m-%d')\n",
    "\n",
    "    # Load historical intervention plans, since inception\n",
    "    hist_ips_df = pd.read_csv(path_to_ips_file,\n",
    "                              parse_dates=['Date'],\n",
    "                              encoding=\"ISO-8859-1\",\n",
    "                              dtype={\"RegionName\": str},\n",
    "                              error_bad_lines=True)\n",
    "\n",
    "    # Add GeoID column that combines CountryName and RegionName for easier manipulation of data\",\n",
    "    hist_ips_df['GeoID'] = hist_ips_df['CountryName'] + '__' + hist_ips_df['RegionName'].astype(str)\n",
    "    # Fill any missing NPIs by assuming they are the same as previous day\n",
    "    for npi_col in NPI_COLS:\n",
    "        hist_ips_df.update(hist_ips_df.groupby(['CountryName', 'RegionName'])[npi_col].ffill().fillna(0))\n",
    "\n",
    "    # Intervention plans to forecast for: those between start_date and end_date\n",
    "    ips_df = hist_ips_df[(hist_ips_df.Date >= start_date) & (hist_ips_df.Date <= end_date)]\n",
    "\n",
    "    # Load historical data to use in making predictions in the same way\n",
    "    # This is the data we trained on\n",
    "    # We stored it locally as for predictions there will be no access to the internet\n",
    "    hist_cases_df = pd.read_csv(DATA_FILE,\n",
    "                                parse_dates=['Date'],\n",
    "                                encoding=\"ISO-8859-1\",\n",
    "                                dtype={\"RegionName\": str,\n",
    "                                       \"RegionCode\": str},\n",
    "                                error_bad_lines=False)\n",
    "    # Add RegionID column that combines CountryName and RegionName for easier manipulation of data\n",
    "    hist_cases_df['GeoID'] = hist_cases_df['CountryName'] + '__' + hist_cases_df['RegionName'].astype(str)\n",
    "    # Add new cases column\n",
    "    hist_cases_df['NewCases'] = hist_cases_df.groupby('GeoID').ConfirmedCases.diff().fillna(0)\n",
    "    # Fill any missing case values by interpolation and setting NaNs to 0\n",
    "    hist_cases_df.update(hist_cases_df.groupby('GeoID').NewCases.apply(\n",
    "        lambda group: group.interpolate()).fillna(0))\n",
    "    # Keep only the id and cases columns\n",
    "    hist_cases_df = hist_cases_df[ID_COLS + CASES_COL]\n",
    "\n",
    "    # Load model\n",
    "    with open(MODEL_FILE, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "\n",
    "    # Make predictions for each country,region pair\n",
    "    geo_pred_dfs = []\n",
    "    for g in ips_df.GeoID.unique():\n",
    "        if verbose:\n",
    "            print('\\nPredicting for', g)\n",
    "\n",
    "        # Pull out all relevant data for country c\n",
    "        hist_cases_gdf = hist_cases_df[hist_cases_df.GeoID == g]\n",
    "        last_known_date = hist_cases_gdf.Date.max()\n",
    "        ips_gdf = ips_df[ips_df.GeoID == g]\n",
    "        past_cases = np.array(hist_cases_gdf[CASES_COL])\n",
    "        past_npis = np.array(hist_ips_df[NPI_COLS])\n",
    "        future_npis = np.array(ips_gdf[NPI_COLS])\n",
    "\n",
    "        # Make prediction for each day\n",
    "        geo_preds = []\n",
    "        # Start predicting from start_date, unless there's a gap since last known date\n",
    "        current_date = min(last_known_date + np.timedelta64(1, 'D'), start_date)\n",
    "        days_ahead = 0\n",
    "        while current_date <= end_date:\n",
    "            # Prepare data\n",
    "            X_cases = past_cases[-NB_LOOKBACK_DAYS:]\n",
    "            X_npis = past_npis[-NB_LOOKBACK_DAYS:]\n",
    "            X = np.concatenate([X_cases.flatten(),\n",
    "                                X_npis.flatten()])\n",
    "\n",
    "            # Make the prediction (reshape so that sklearn is happy)\n",
    "            pred = model.predict(X.reshape(1, -1))[0]\n",
    "            pred = max(0, pred)  # Do not allow predicting negative cases\n",
    "            # Add if it's a requested date\n",
    "            if current_date >= start_date:\n",
    "                geo_preds.append(pred)\n",
    "                if verbose:\n",
    "                    print(f\"{current_date.strftime('%Y-%m-%d')}: {pred}\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"{current_date.strftime('%Y-%m-%d')}: {pred} - Skipped (intermediate missing daily cases)\")\n",
    "\n",
    "            # Append the prediction and npi's for next day\n",
    "            # in order to rollout predictions for further days.\n",
    "            past_cases = np.append(past_cases, pred)\n",
    "            past_npis = np.append(past_npis, future_npis[days_ahead:days_ahead + 1], axis=0)\n",
    "\n",
    "            # Move to next day\n",
    "            current_date = current_date + np.timedelta64(1, 'D')\n",
    "            days_ahead += 1\n",
    "\n",
    "        # Create geo_pred_df with pred column\n",
    "        geo_pred_df = ips_gdf[ID_COLS].copy()\n",
    "        geo_pred_df['PredictedDailyNewCases'] = geo_preds\n",
    "        geo_pred_dfs.append(geo_pred_df)\n",
    "\n",
    "    # Combine all predictions into a single dataframe\n",
    "    pred_df = pd.concat(geo_pred_dfs)\n",
    "\n",
    "    # Drop GeoID column to match expected output format\n",
    "    pred_df = pred_df.drop(columns=['GeoID'])\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! PLEASE DO NOT EDIT. THIS IS THE OFFICIAL COMPETITION API !!!\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-s\", \"--start_date\",\n",
    "                        dest=\"start_date\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"Start date from which to predict, included, as YYYY-MM-DD. For example 2020-08-01\")\n",
    "    parser.add_argument(\"-e\", \"--end_date\",\n",
    "                        dest=\"end_date\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"End date for the last prediction, included, as YYYY-MM-DD. For example 2020-08-31\")\n",
    "    parser.add_argument(\"-ip\", \"--interventions_plan\",\n",
    "                        dest=\"ip_file\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"The path to an intervention plan .csv file\")\n",
    "    parser.add_argument(\"-o\", \"--output_file\",\n",
    "                        dest=\"output_file\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"The path to the CSV file where predictions should be written\")\n",
    "    args = parser.parse_args()\n",
    "    print(f\"Generating predictions from {args.start_date} to {args.end_date}...\")\n",
    "    predict(args.start_date, args.end_date, args.ip_file, args.output_file)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: predict.py [-h] -s START_DATE -e END_DATE -ip IP_FILE -o OUTPUT_FILE\n",
      "predict.py: error: the following arguments are required: -ip/--interventions_plan\n"
     ]
    }
   ],
   "source": [
    "!python predict.py -s '2020-08-01' -e '2020-08-04' -ip 'covid_xptorical_ip.csv' -o 'predictions/2020-08-01_2020-08-04.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
